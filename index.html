<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark</title>
  <link rel="icon" type="image/png" href="assets/arab_logo.png" />
  <link rel="stylesheet" href="./static/css/bulma.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css" />
  <link rel="stylesheet" href="style.css" />

  <style>
    .button {
      display: inline-flex;
      align-items: center;
      justify-content: center;
      padding: 10px 18px;
      border-radius: 30px;
      background-color: #222;
      color: white;
      font-size: 16px;
      text-decoration: none;
      border: none;
      transition: background-color 0.3s ease;
    }

    .button:hover {
      background-color: #eeeeee;
    }

    .button .icon {
      margin-right: 8px;
    }

    .header-container {
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 15px;
    }

    .logo {
      height: 50px;
      width: auto;
      display: inline-block;
    }

    .title {
      font-size: 3em;
      margin: 0;
      background: linear-gradient(to right, #333333, #666666);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      font-weight: bold;
      font-family: Geneva, Tahoma, sans-serif;
    }

    .title2 {
      font-size: 2em;
      margin: 0px 0;
      background: linear-gradient(to right, #585858, #666666);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      font-family: Geneva, Tahoma, sans-serif;
    }

    .title3 {
      font-size: 1.5em;
      margin: 0px 0;
      background: linear-gradient(to right, #585858, #666666);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      font-family: Geneva, Tahoma, sans-serif;
    }

    .logos {
      display: flex;
      justify-content: center;
      align-items: center;
      gap: 20px;
      margin: 20px 0;
    }

    .logos img {
      height: 40px; /* FIXED: Reasonable footer logo size */
      width: auto;
      object-fit: contain;
    }

    /* Additional styling below kept the same... */
  </style>
</head>
<body>
           <div class="header-container">
                <img src="assets/arab_logo.png" alt="logo" class="logo">
                <h1 class="title">ARB</h1>
            </div>
            <h1 class="title2">A Comprehensive Arabic Reasoning Benchmark</h1>
            <!-- Authors -->
            <div class="authors">
                <p>
                    Sara Ghaboural<sup style="color:#3399FF;">1*</sup>, 
                    Ketan More<sup style="color:#3399FF;">1*</sup>, 
                    Wafa Alghallabi<sup style="color:#3399FF;">1</sup>, 
                    Omkar Thawakar<sup style="color:#3399FF;">1</sup>, 
                    Jorma Laaksonen<sup style="color:#FF66B3">2</sup>, <br>
                    Hisham Cholakkal<sup style="color:#3399FF;">1, </sup>
                    Salman Khan<sup style="color:#3399FF;">1, </sup><sup style="color:#4CB5AE;">3</sup>,
                    Rao M. Anwer<sup style="color:#3399FF;">1, </sup><sup style="color:#FF66B3;">2</sup>,
                </p>
                <p>
                    <sup style="color:#3399FF;">1</sup>Mohamed bin Zayed University of AI, 
                     <sup style="color:#FF66B3;">2</sup>Aalto University
                    <sup style="color:#4CB5AE;">3</sup>Australian National University,
                </p>
            </div>

            <div class="publication-links has-text-centered">
                <span class="link-block">
                    <a href="https://github.com/mbzuai-oryx/ARB/blob/main/assets/ARB_22_05_2025.pdf" class="external-link button">
                        <img src="assets/pdf.png" alt="PDF">
                        <span>Paper</span>
                    </a>
                </span>
                <span class="link-block">
                    <a href="https://arxiv.org/abs/2502.14865" class="external-link button">
                        <img src="assets/arxiv_1.png" alt="arXiv">
                        <span>arXiv</span>
                    </a>
                </span>
                <span class="link-block">
                    <a href="https://github.com/mbzuai-oryx/ARB." class="external-link button">
                        <img src="assets/git_1.png" alt="GitHub">
                        <span>Code</span>
                    </a>
                </span>
                <span class="link-block">
                    <a href="https://huggingface.co/datasets/MBZUAI/ARB" class="external-link button">
                        <span class="icon" style="font-size:18px">ü§ó</span>
                        <span>Dataset</span>
                        </a>
                    </span>
            </div>
    
          
            <!-- Introductory Figure -->
            <div class="fig-intro section-spacing">
                 <h1 class="title3">ARB: A Comprehensive Arabic Multimodal Reasoning Benchmark </h1>
                <img src="assets/arb_sample_intro.png" alt="ARB Overview">
                <p class="fig-caption">
                    <p>
               <strong>Figure 1. ARB Scope and Diversity </strong>ARB comprises a wide array of multimodal reasoning samples, each combining a visual input with an Arabic question and detailed step-by-step reasoning with actions taken by step. The dataset spans 11 distinct domains, including visual reasoning, OCR and document understanding, chart and diagram interpretation, mathematical and logical inference, scientific and medical analysis, cultural and historical interpretation, remote sensing, agricultural image analysis, and complex visual perception‚Äîcapturing the linguistic richness, cultural depth, and cross-domain complexity essential for evaluating reasoning in Arabic.
                </p>
            </div>

 
    
            
            <!-- Abstract Section -->
            <div class="abstract-section section-spacing">
                <h2 class="title2">Abstract</h2>
                <p>
                   As Large Multimodal Models (LMMs) become more capable, there is growing interest in evaluating their reasoning processes alongside their final outputs. However, most benchmarks remain focused on English, overlooking languages with rich linguistic and cultural contexts, such as Arabic. To address this gap, we introduce the Comprehensive Arabic Multimodal Reasoning Benchmark (ARB), the first benchmark designed to evaluate step-by-step reasoning in Arabic across both textual and visual modalities. ARB spans 11 diverse domains, including visual reasoning, document understanding, OCR, scientific analysis, and cultural interpretation. It comprises 1,356 multimodal samples paired with 5,119 human-curated reasoning steps and corresponding actions. We evaluated 12 state-of-the-art open- and closed-source LMMs and found persistent challenges in coherence, faithfulness, and cultural grounding. ARB offers a structured framework for diagnosing multimodal reasoning in underrepresented languages and marks a critical step toward inclusive, transparent, and culturally aware AI systems. We release the benchmark, rubric, and code to support future research and reproducibility.
                </p>
            </div>
    
            <!-- Pipeline Figure -->
            <div class="fig-intro section-spacing">
                 <h1 class="title3">ARB Construction Pipeline</h1>
                <img src="assets/arb_pipeline.png" alt="ARBpipeline">
                <p class="fig-caption">
                     <p>
                  <strong>Figure 2. ARB Pipeline </strong>The figure illustrates the ARB pipeline for evaluating Arabic multimodal reasoning in LMMs. It begins with data collection across 11 domains‚Äîsuch as medical imaging, historical interpretation, visual reasoning, and agriculture‚Äîsourced from curated datasets (e.g., VRC-Bench, CAMEL-Bench), synthetic content, tool-augmented outputs, and web scraping. Data is generated across five categories: English reasoning chains, Arabic Q\&A, English captions, synthetic samples, and tool-enhanced content. Reasoning steps are refined via human-in-the-loop feedback and filtered for logical consistency and cultural alignment. The benchmark supports fine-grained evaluation of open- and closed-source models on Arabic step-by-step reasoning.
                     </p>
                
        <!-- Collection Figure -->
            <div class="fig-intro section-spacing">
                 <h1 class="title3">ARB Data Collection</h1>
                <img src="assets/arb_collection.png" alt="ARBcollect">
                <p class="fig-caption">
                    <p>
                   <strong>Figure 3. ARB Data Collection </strong>Overview of the ARB Data Collection, Generation and Verification Framework.} The ARB benchmark is constructed from five primary data sources: (1) English reasoning benchmarks, (2) Arabic question‚Äìanswer benchmarks, (3) English-captioned datasets, (4) Synthetic data, and (5) Tool-augmented data. All data undergoes iterative refinement through human-in-the-loop feedback and validation by native Arabic speakers to ensure cultural and linguistic fidelity.  
                    </p> 

                    
     <div class="abstract-section section-spacing">
             <h2 class="title2">Quantitative Evaluation and Results</h2>
                   </div>

<h5>
<table>
    <thead>
        <tr style="background-color: #c5d9d9; color: white;">
            <th>Model</th>
            <th>GPT-4o</th>
            <th>GPT-4o-min</th>
            <th>GPT-4.1</th>
            <th>o4-mini</th>
            <th>Gemini 1.5 Pro</th>
            <th>Gemini 2.0 Flash</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Final Answer (%)</td>
            <td><b>60.22üèÖ</b></td>
            <td>52.22</td>    
            <td>59.43</td>
            <td>58.93</td>
            <td>56.70</td>
            <td>57.80</td>
        </tr>
        <tr>
            <td>Reasoning Steps (%)</td>
            <td>64.29</td>     
            <td>61.02</td>
            <td>80.41</td>
            <td><b>80.75üèÖ</b</td>
            <td>64.34</td>
            <td>64.09</td>
        </tr>
    </tbody>
</table></h5>
<p>
 <h6>
       <em>  <strong>Table1: Stepwise Evaluation Using LLM-as-Judge- Closed-Source Modles.</strong>  Comparison of closed-weight models based on final answer accuracy and aggregated quality scores of reasoning steps, using our LLM-as-Judge framework with Arabic prompts and evaluation metrics. The evaluation follows a reference-based, attribute-level protocol for assessing reasoning quality.</em>
 </h6>  
</p>

<br>
<br>
<table>
    <thead>
        <tr style="background-color: #d4ebdb; color: white;">
            <th>Model</th>
            <th>Qwen2.5VL-7b</th>
            <th>Llama-3.2-11B-Vis-Inst.</th>
            <th>AIN</th>
            <th>Llama-4-Scout-17Bx16E</th>
            <th>Aya-Vision-8B</th>
            <th>InternVl3-8B</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Final Answer (%)</td>
            <td>37.02</td>     
            <td>25.58</td>    
            <td>27.35</td>
            <td><b>48.52üèÖ</b></td>
            <td>28.81</td>
            <td>31.04</td>
        </tr>
        <tr>
            <td>Reasoning Steps (%)</td>
            <td>64.03</td>          
            <td>53.20</td>
            <td>52.77</td>
            <td><b>77.70üèÖ</b></td>
            <td>63.64</td>
            <td>54.50</td>
        </tr>
    </tbody>
</table></h5>
<p>
 <h6>
       <em>  <strong>Table2: Stepwise Evaluation Using LLM-as-Judge- Open-Source Modles.</strong>  Comparison of closed-weight models based on final answer accuracy and aggregated quality scores of reasoning steps, using our LLM-as-Judge framework with Arabic prompts and evaluation metrics. The evaluation follows a reference-based, attribute-level protocol for assessing reasoning quality.</em>
 </h6>  
</p>
</div>

   <div class="abstract-section section-spacing">
                       <h2 class="title2">Qulitative Examples</h2>
                   </div>

                
  <!-- Qualitative Errors in Closed-Source Models -->
            <div class="fig-intro section-spacing">
                 <h1 class="title3">Qualitative Errors in Closed-Source Models</h1>
                <img src="assets/closed_ex.png" alt="ARBclosed">
                <p class="fig-caption">
                   <strong>Figure 5. Qualitative Errors in Closed-Source Models. </strong> This figure highlights reasoning failures by closed-source LMMs across various Arabic multimodal tasks. Common issues include incorrect numerical comparisons, invalid assumptions, misinterpreted constraints, and logically inconsistent step sequences. These errors often lead to incorrect conclusions despite the appearance of structured reasoning, underscoring the limitations of current closed models when operating in Arabic.
                    
  <!-- Qualitative Errors in Open-Source Models -->
            <div class="fig-intro section-spacing">
                 <h1 class="title3">Qualitative Errors in Open-Source Models</h1>
                <img src="assets/open_ex.png" alt="ARBopen">
                <p class="fig-caption">
      
                    <strong>Figure 6. Qualitative Errors in Open-Source Models. </strong>This figure showcases common reasoning flaws in open-source LMMs across diverse Arabic multimodal tasks. Errors include incomplete reasoning steps, inconsistent logic, and hallucinated interpretations not grounded in the input. These issues often result in incorrect answers or unreliable outputs, reflecting the challenges open models face in structured Arabic reasoning.
                    
    <!-- Cross-Lingual Reasoning Comparison  -->
            <div class="fig-intro section-spacing">
                 <h1 class="title3"> Cross-Lingual Reasoning Comparison </h1>
                <img src="assets/comparison.png" alt="ARBcross">
                <p class="fig-caption">
                   <strong>Figure 7. Cross-Lingual Reasoning Comparison (Arabic vs. English).  </strong>This figure compares LMMs (GPT-4o) reasoning steps in Arabic and English for the same visual task. In the Arabic version, the model misinterprets structural constraints, yellow highlights incorrect assumptions about equal line counts across boxes, green emphasizes miscounted lines within the boxes, and cyan marks an irrelevant search for a box with exactly 4 lines. These reasoning flaws lead to the wrong answer (C). In contrast, the English reasoning is structured, accurate, and constraint-aware, correctly identifying the answer (A), highlighting the performance gap in Arabic.
                    
             </div>  
  <footer class="footer">
    <div class="logos">
      <img src="assets/IVAL_logo.png" alt="ival" />
      <img src="assets/Oryx_logo.png" alt="oryx" />
      <img src="assets/MBZUAI_logo.png" alt="mbz" />
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from
            <a href="https://nerfies.github.io/">Nerfies</a> and
            <a href="https://mathvista.github.io/">MathVista</a>, licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
              Creative Commons Attribution-ShareAlike 4.0 International License</a
            >.
          </p>
        </div>
      </div>
    </div>
  </footer>
</body>
</html>
